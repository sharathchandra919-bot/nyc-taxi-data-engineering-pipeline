{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b5f095-2e2f-4817-a431-60e99c62a996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|       1| 2021-07-01 00:30:52|  2021-07-01 00:35:36|                 N|         1|          74|         168|              1|          1.2|        6.0|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         7.3|           2|        1|                 0.0|\n|       2| 2021-07-01 00:25:36|  2021-07-01 01:01:31|                 N|         1|         116|         265|              2|        13.69|       42.0|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|        43.3|           2|        1|                 0.0|\n|       2| 2021-07-01 00:05:58|  2021-07-01 00:12:00|                 N|         1|          97|          33|              1|         0.95|        6.5|  0.5|    0.5|      2.34|         0.0|     NULL|                  0.3|       10.14|           1|        1|                 0.0|\n|       2| 2021-07-01 00:41:40|  2021-07-01 00:47:23|                 N|         1|          74|          42|              1|         1.24|        6.5|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         7.8|           2|        1|                 0.0|\n|       2| 2021-07-01 00:51:32|  2021-07-01 00:58:46|                 N|         1|          42|         244|              1|          1.1|        7.0|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         8.3|           2|        1|                 0.0|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\nonly showing top 5 rows\nroot\n |-- VendorID: integer (nullable = true)\n |-- lpep_pickup_datetime: timestamp (nullable = true)\n |-- lpep_dropoff_datetime: timestamp (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- RatecodeID: integer (nullable = true)\n |-- PULocationID: integer (nullable = true)\n |-- DOLocationID: integer (nullable = true)\n |-- passenger_count: integer (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- ehail_fee: string (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- payment_type: integer (nullable = true)\n |-- trip_type: integer (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"workspace.default.taxi_tripdata\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b3fdcc-eb4e-482f-81be-6c748fc14933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n|       2| 2008-12-31 23:12:53|  2008-12-31 23:27:09|                 N|         1|         166|          75|              1|         1.88|       10.5|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|        11.3|           2|        1|                 0.0|\n|       2| 2009-01-01 00:13:47|  2009-01-01 00:15:53|                 N|         1|          41|         166|              1|         0.52|        3.5|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|         4.3|           2|        1|                 0.0|\n|       2| 2009-01-01 00:17:19|  2009-01-01 06:01:13|                 N|         1|          75|          23|              1|        28.76|       80.5|  0.5|    0.5|       0.0|        6.55|     NULL|                  0.3|       88.35|           1|        1|                 0.0|\n|       2| 2009-01-01 01:03:17|  2009-01-01 01:07:24|                 N|         1|          75|          74|              1|         1.17|        5.5|  0.0|    0.5|       0.0|         0.0|     NULL|                  0.3|         6.3|           2|        1|                 0.0|\n|       2| 2009-01-01 01:45:46|  2009-01-01 01:47:06|                 N|         1|         193|         193|              1|          0.0|        0.0|  0.0|    0.0|       0.0|         0.0|     NULL|                  0.0|         0.0|           2|        1|                 0.0|\n+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number,count\n",
    "\n",
    "bronze_df = spark.table(\"workspace.default.taxi_tripdata\")\n",
    "\n",
    "windows = Window.partitionBy(\"lpep_pickup_datetime\",\"PULocationID\",\"DOLocationID\").orderBy(col(\"lpep_pickup_datetime\").desc())\n",
    "                                                                                                      \n",
    "                                                                                                                                                                                               \n",
    "silver_df = bronze_df.withColumn(\"row_number\", row_number().over(windows)).filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "silver_df.show(5)\n",
    "# bronze_df.select(col(\"lpep_pickup_datetime\")).count()\n",
    "# silver_df.select(col(\"lpep_pickup_datetime\")).count()\n",
    "\n",
    "silver_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"workspace.default.silver_taxi_tripdata\")                                                           \n",
    "                                                                                                      \n",
    "# silver_df.groupBy(\"VendorID\",\"lpep_pickup_datetime\",\"PULocationID\",\"DOLocationID\").agg(count(\"*\").alias(\"cnt\")).filter(\"cnt > 1\").show(5)                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8497a151-4b18-4f49-bfa1-b0444d7c23a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n|pickup_nulls|not_nulls|\n+------------+---------+\n|       32518|    51173|\n+------------+---------+\n\n+-----------+\n|pickup_null|\n+-----------+\n|      32508|\n+-----------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "83691"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bronze_df.filter(\n",
    "#     col(\"lpep_pickup_datetime\") == \"2021-07-01 00:13:00\"\n",
    "# ).select(\n",
    "#     \"VendorID\",\n",
    "#     \"lpep_pickup_datetime\",\n",
    "#     \"PULocationID\",\n",
    "#     \"DOLocationID\"\n",
    "# ).show(truncate=False)\n",
    "\n",
    "# bronze_df.filter(col(\"VendorID\").isnull()).count()\n",
    "\n",
    "from pyspark.sql.functions import sum, when\n",
    "\n",
    "bronze_df.select(\n",
    "    sum(when(col(\"VendorID\").isNull(), 1)).alias(\"pickup_nulls\"),\n",
    "    sum(when(col(\"VendorID\").isNotNull(), 1)).alias(\"not_nulls\"),\n",
    "    # sum(when(col(\"DOLocationID\").isNull(), 1)).alias(\"do_nulls\")\n",
    ").show()\n",
    "\n",
    "silver_df.select(sum(when(col(\"VendorID\").isNull(), 1)).alias(\"pickup_null\")).show()\n",
    "\n",
    "bronze_df.select(col(\"lpep_pickup_datetime\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc0b801-228a-4e66-8cd5-0a98a7e4758c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, lpep_pickup_datetime: timestamp, lpep_dropoff_datetime: timestamp, store_and_fwd_flag: string, RatecodeID: int, PULocationID: int, DOLocationID: int, passenger_count: int, trip_distance: double, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, ehail_fee: string, improvement_surcharge: double, total_amount: double, payment_type: int, trip_type: int, congestion_surcharge: double]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watermark = spark.table(\"workspace.default.silver_taxi_tripdata\").agg({\"lpep_pickup_datetime\":\"max\"}).collect()[0][0]\n",
    "\n",
    "bronze_df.filter(col(\"lpep_pickup_datetime\") > watermark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd132ece-4f4c-4a59-a35e-9feef79f84f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "62513"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,expr\n",
    "last_watermark = \"2021-07-10 00:00:00\"\n",
    "\n",
    "incremental_df = bronze_df.filter(col(\"lpep_pickup_datetime\") > expr(f\"timestamp('{last_watermark}') - INTERVAL 1 DAY\"))\n",
    "\n",
    "incremental_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31bcf6aa-5155-44e2-b8ee-03df808fcb21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "windows = Window.partitionBy(\"lpep_pickup_datetime\",\"PULocationID\",\"DOLocationID\").orderBy(col(\"VendorID\").isNull(),col(\"lpep_pickup_datetime\").desc())\n",
    "\n",
    "incremental_dedup_df = incremental_df.withColumn(\"row_number\", row_number().over(windows)).filter(col(\"row_number\") == 1).drop(\"row_number\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80b248a-d1c3-45ff-8d13-ceaa0dd019c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADF watermark: 2021-07-10 00:00:00\nADF would update watermark to: 2021-08-01 19:55:44\n"
     ]
    }
   ],
   "source": [
    "incremental_dedup_df.createOrReplaceTempView(\"incremental_updates\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO workspace.default.silver_taxi_tripdata t\n",
    "USING incremental_updates s\n",
    "ON  t.lpep_pickup_datetime = s.lpep_pickup_datetime\n",
    "AND t.PULocationID = s.PULocationID\n",
    "AND t.DOLocationID = s.DOLocationID\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "new_watermark = incremental_dedup_df.agg({\"lpep_dropoff_datetime\":\"max\"}).collect()[0][0]\n",
    "\n",
    "incremental_dedup_df.count()\n",
    "\n",
    "spark.table(\"workspace.default.silver_taxi_tripdata\").count()\n",
    "\n",
    "print(\"ADF watermark:\", last_watermark)\n",
    "\n",
    "print(\"ADF would update watermark to:\", new_watermark)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de0b6e0-d768-4f71-b3e8-190404367d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gold layer\n",
    "\n",
    "from pyspark.sql.functions import col,coalesce,lit,to_date,sum,avg,count\n",
    "\n",
    "silver_df = spark.table(\"workspace.default.silver_taxi_tripdata\")\n",
    "\n",
    "gold_df = silver_df.withColumn(\"pickup_date\",to_date(col(\"lpep_pickup_datetime\"))).withColumn(\"VendorID_clean\",coalesce(col(\"VendorID\").cast(\"string\"),lit(\"UNKNOWN\")))\n",
    "\n",
    "gold_daily_metrics = gold_df.groupBy(\"pickup_date\",\"VendorID_clean\").agg(count(\"*\").alias(\"cnt\"),sum(\"total_amount\").alias(\"total_amount\"),avg(\"trip_distance\").alias(\"avg_trip_distance\"))\n",
    "\n",
    "gold_daily_metrics.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"workspace.default.gold_taxi_tripdata\")\n",
    "\n",
    "spark.table(\"workspace.default.gold_taxi_tripdata\").count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks_project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}